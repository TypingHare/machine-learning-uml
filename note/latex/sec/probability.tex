\section{Probability}
\label{sec:probability}

\subsection{Bernoulli and Binomial Distribution}
\label{subsec:bernoulli-and-binomial-distribution}

The probability distribution over a binary random variable $x \in \{ 0, 1 \}$ can be described using the \textbf{Bernoulli distribution} as:

\[
    \text{Bern}(x | p) = p^x(1-p)^{1-x}
\]

Consider $n$ independent variables $X_1, X_2, \dots, X_n$, and each follows $\text{Bern}(p)$.
The sum of these variables is called a binomial random variable:

\[
    X = \sum_{i}^{n}{X_i}
\]

It follows the \textbf{binomial distribution}:

\[
    \text{Bin}(m | n, p) = {n \choose m} p^m(1 - p)^{n - m}
\]

Where $n$ is the number of trials, and $m$ is the number of successes.
The mean and the variance of the Binomial distribution are given by

\begin{gather*}
    \mathbb{E}[m] = np \\
    \mathbb{V}[m] = np(1 - p)
\end{gather*}

\subsection{Maximum Likelihood Estimation}
\label{subsec:maximum-likelihood-estimation}

\textbf{Maximum Likelihood Estimation (MLE)} is a statistical method used to estimate the parameters of a probability by maximizing the likelihood function.

Consider $X \sim \text{Bern}(n, \mu)$ and a data set $\mathcal{D} = \{ x_i \}_1^n$ is observed.
The likelihood function can be formulated under the assumption that observations are independently sampled from the distribution $p(x | \mu)$:

\[
    p(\mathcal{D} | \mu) = \prod_{i = 1}^{n}{ p(x_i | \mu) }
\]

The log-likelihood can be written as

\[
    \ln{ p(\mathcal{D} | \mu) }
    = \sum_{i = 1}^{n}{ \ln{ p(x_i | \mu) } }
    = \sum_{i = 1}^{n}{ x_i \ln \mu + (1 - x_i) \ln (1 - \mu) }
\]

Find the partial derivative of $\ln { p(\mathcal{D} | \mu) }$ w.r.t $\mu$ and set it to zero, we obtain

\[
    \mu_{ML} = \frac{1}{n}\sum_{i}^{n}{ x_i }
\]

Therefore, the MLE of the mean is equivalent to the sample mean.

\subsection{Beta Distribution}
\label{subsec:beta-distribution}

The Beta distribution is a continuous probability distribution defined on the interval $[0, 1]$:

\[
    f(x; \alpha, \beta) = \frac{x^{\alpha - 1}(1 - x)^{\beta - 1}}{B(\alpha, \beta)}
\]

Where $B(\alpha, \beta)$ is the \textbf{Beta function}, defined as:

\[
    B(\alpha, \beta)
    = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}
    = \int_{0}^{1}{ t^{\alpha - 1}(1-t)^{\beta - 1} dt }
\]

\textbf{Bayesian inference} is a method of statistical inference that updates our belief about unknown parameter using observed data, following Bayes' theorem:

\[
    P(\theta | X) = \frac{ P(X | \theta) P(\theta) }{ P(X) }
\]

Where:
    \begin{itemize}
        \item { $P(\theta | X)$ is the \textbf{posterior probability} of the parameter $\theta$ given the observed data $X$. }
        \item { $P(X | \theta)$ is the \textbf{likelihood}, the probability of observing the data given $\theta$. }
        \item { $P(\theta)$ is the \textbf{prior}, our belief about $\theta$ before seeing the data. }
        \item { $P(X)$ is the \textbf{evidence}, a constant ensuring the total probability sums to $1$. }
    \end{itemize}

Because the evidence $P(X)$ is a normalizing constant ensuring that the total probability sums to $1$, we can write:

\[
    P(\theta | X) \propto P(X|\theta)P(\theta)
\]

The beta distribution is the \textbf{conjugate prior probability distribution} for the Bernoulli, binomial, negative binomial, and geometric distributions.

Consider $X \sim \text{Bern}(n, \mu)$ and a data set $\mathcal{D} = \{ x_i \}_1^n$ is observed.
Assume that the prior is a Beta distribution:

\[
    \text{Beta}(\mu | a_0, b_0) = \frac{1}{B(a_0, b_0)} \mu^{a_0 - 1}(1 - \mu)^{b_0 - 1}
\]

We have:

\[
    \begin{align*}
        p(\mu | a_0, b_0, \mathcal{D})
        &\propto p(\mathcal{D} | \mu)\text{Beta}(\mu | a_0, b_0) \\
        &= \left( \prod_{i = 1}^{n}{ \mu_i^x(1-\mu)^{1-x_i} } \right) \cdot \frac{1}{B(a_0, b_0)} x^{a_0 - 1}(1 - x)^{b_0 - 1} \\
        &= \mu^m(1-\mu)^{n - m} \cdot \frac{1}{B(a_0, b_0)} \mu^{a_0 - 1}(1 - \mu)^{b_0 - 1} \\
        &\propto \mu^{m + a_0 - 1}(1 - \mu)^{n - m + b_0 - 1} \\
        &\propto \text{Beta}(\mu | m + a0, n - m + b_0)
    \end{align*}
\]

\subsection{Gaussian Distribution}
\label{subsec:gaussian-distribution}

The \textbf{Gaussian distribution} (also known as the \textbf{normal distribution}) for a scalar random variable $x$ is defined as:

\[
    \mathcal{N}(x \mid \mu, \sigma^2)
    = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{(x - \mu)^2}{2\sigma^2}\right\}
\]

where $\mu$ is the mean and $\sigma^2$ is the variance.

For a $D$-dimensional random vector $\mathbf{x}$, the \textbf{multivariate Gaussian distribution} is:

\[
    \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma})
    = \frac{1}{(2\pi)^{D/2}|\boldsymbol{\Sigma}|^{1/2}} \exp\left\{-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\mathbf{x} - \boldsymbol{\mu})\right\}
\]

where $\boldsymbol{\mu}$ is the mean vector and $\boldsymbol{\Sigma}$ is the covariance matrix.

Given independent and identically distributed observations $\{x_i\}_{i=1}^n$, the likelihood function is:

\[
    p(\mathbf{x} \mid \mu, \sigma^2) = \prod_{i = 1}^{n}\mathcal{N}(x_i \mid \mu, \sigma^2)
\]

Taking the natural logarithm of the likelihood function yields:

\[
    \ln{p(\mathbf{x} \mid \mu, \sigma)} = -\frac{1}{\sigma^2}\sum_{n=1}^{n}(x_n - \mu)^2 - \frac{n}{2}\ln \sigma^2 - \frac{n}{2}(2 \pi)
\]

To find the maximum likelihood estimate of $\mu$, we take the derivative w.r.t. $\mu$ and set it equal to zero:

\[
    \frac{\partial}{\partial \mu} \ln p(\mathbf{x} \mid \mu, \sigma)
    = \frac{2}{\sigma^2}\sum_{i=1}^{n}{(x_i - \mu)}
\]

Solving for $\mu$:

\[
    \mu = \frac{1}{n}\sum_{i=1}^{n}x_i
\]

Thus, the maximum likelihood estimate for $\mu$ is the sample mean.
Likewise, the maximum likelihood estimate for $\sigma^2$ is the sample variance.
In conclusion:

\[
    \mu_{ML} = \frac{1}{N}\sum_{n=i}^{n}{x_i}
    \quad
    \sigma^2_{ML} =  \frac{1}{n}\sum_{i = 1}^{n}{ (x_i - \mu_{ML})^2 }
\]